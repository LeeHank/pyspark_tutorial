{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什麼是 Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark是用來做cluster computing的一個平台(platform)  \n",
    "* 實務上來說，cluster會被 hosted 在一個遠端的電腦(稱為 master)，他的任務是做資料分割和計算。  \n",
    "* master 會和 cluster 裡面的其他電腦連接再一起(這些電腦叫 worker)，master會把data切割成好幾份資料，寄送給workers，workers做完計算後，再把結果送回給master統整。  \n",
    "* 所以，我們把原本超巨大的資料，切成好n塊，丟到n個nodes中幫我們計算，就可以處理掉big data的問題，因為每個node其實只要處理小小的資料就好  \n",
    "* 所以，both data processing and computation are performed in parallel over the nodes in the cluster  \n",
    "* 所以，有需要用spark的兩個條件是：  \n",
    "  * 我的資料實在大到一台電腦無法處理，要綁多台電腦一起處理才行  \n",
    "  * 我想做的運算，是可以平行化處理的，我才能把工作分散到多個電腦中一起做  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Spark (RDD vs DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* spark 的核心資料結構是 Resilient Distributed Dataset (RDD).  \n",
    "* 這是一個低階物件，可讓 Spark 將資料切割到多個 nodes 去計算.   \n",
    "* 要使用 RDD 的話，要先instance 一個 `SparkContext` 物件 (e.g. `sc = pyspark.SparkContext(master = local[2])`)。之後就可以用 sc 這個物件，去 load/create RDD 資料，之後就可以做運算了。  \n",
    "* 但 RDDs 不太好寫。比如說，要達到同一個目的，你可以有多種寫法，但效能差異很大，你必須很有sense的選擇好的寫法，才會有好的performance. \n",
    "* 所以，後來又推出了 `Spark DataFrame` 這種資料結構。他是建立在 RDDs 上面的資料結構，被設計的像 SQL table，不只好理解，好寫，而且效能已經被最佳化的很好了  \n",
    "* 要使用 `Spark DataFrame` 的話，要先 instance 一個 `SparkSession` 物件 (e.g. `spark = pyspark.sql.SparkSession.builder.getOrCreate()`)，之後就可以用 spark 這個物件，去 create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立 SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x119430af0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀檔 (轉換成 Spark DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 要建立 Spark DataFrame 的方式有很多，我們先介紹最簡單的：讀 local 的 csv 檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights = spark.read.csv(\"flights_small.csv\") # 此時的 flights，已經是 Spark DataFrame 這種資料結構\n",
    "flights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，當我想 print `flights` 時，他並沒有真的 print 出一個 table 給我。\n",
    "* 因為 Spark DataFrame 是 lazy evaluztion，所以他只會給你看基本資訊，直到你叫他 show 出結果時，他才會真的執行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "| _c0|  _c1|_c2|     _c3|      _c4|     _c5|      _c6|    _c7|    _c8|   _c9|  _c10|_c11|    _c12|    _c13|_c14|  _c15|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5) # show 前五筆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| _c0|\n",
      "+----+\n",
      "|year|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "|2014|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.select(\"_c0\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 一但建立好 SparkSession 後，就可以看看我們的 cluster 裡面已經有哪些 data 可以用了. \n",
    "* SparkSession有個 attribute 叫 `catalog`， 裡面就會列出 cluster 內有的所有 data  \n",
    "* This attribute has a few methods for extracting different pieces of information.\n",
    "* One of the most useful is the .listTables() method, which returns the names of all the tables in your cluster as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 架構"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](spark_figure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 如上圖，那朵大大的雲，就是Spark的世界，我們想要運用它強大的平行運算能力。那第一步，就是先去和這朵雲建立 connection. \n",
    "* \n",
    "\n",
    "我們從 `user` 和上面的 `SparkContext`，以及右邊的 `SparkSession` 開始看\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x119430af0>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark DataFrame 轉成 pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 當我們在 spark 上把資料篩選成比較小的資料集時，就可以把它轉成 pandas ，就有更多工具可以做處理了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
    "\n",
    "# Run the query\n",
    "flight_counts = spark.sql(query)\n",
    "\n",
    "# Convert the results to a pandas DataFrame\n",
    "pd_counts = flight_counts.toPandas()\n",
    "\n",
    "# Print the head of pd_counts\n",
    "print(pd_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas DataFrame 轉成 spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_temp = pd.DataFrame(np.random.random(10), columns = [\"col1\"])\n",
    "\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = spark.createDataFrame(pd_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n",
      "[Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# Examine the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.797822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.340544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.852814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.924986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.745240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.403150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.575103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.743448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.801872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.567244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       col1\n",
       "0  0.797822\n",
       "1  0.340544\n",
       "2  0.852814\n",
       "3  0.924986\n",
       "4  0.745240\n",
       "5  0.403150\n",
       "6  0.575103\n",
       "7  0.743448\n",
       "8  0.801872\n",
       "9  0.567244"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              col1|\n",
      "+------------------+\n",
      "|0.7978216511918942|\n",
      "|0.8528142311265194|\n",
      "|0.9249857505093086|\n",
      "|0.7452404560538725|\n",
      "|0.5751031435671135|\n",
      "|0.7434483360774634|\n",
      "|0.8018722142974994|\n",
      "|0.5672444145515884|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select col1 from temp where col1 > 0.5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}